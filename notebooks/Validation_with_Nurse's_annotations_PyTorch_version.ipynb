{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DS73XUC7QXyA"
      },
      "source": [
        "## 1. Load the libraries and datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndat5KH-oiMw",
        "outputId": "958acfbe-cb18-4a4f-bbfb-4ae03071c85f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n",
            "Done\n"
          ]
        }
      ],
      "source": [
        "%pip install ujson > /dev/null\n",
        "\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "import gdown\n",
        "import os\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "import numpy as np\n",
        "import cv2\n",
        "from matplotlib import pyplot as plt\n",
        "import shutil\n",
        "import random\n",
        "from numpy import asarray\n",
        "\n",
        "# PyTorch Dependencies\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import gc\n",
        "import torch.optim as optim\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "print(\"Done\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_5-nncz9afo"
      },
      "outputs": [],
      "source": [
        "# Download annotated subsets\n",
        "!unzip -qn '/content/gdrive/MyDrive/validation_data.zip' > /dev/null\n",
        "\n",
        "# Download original images\n",
        "!unzip -qn '/content/gdrive/MyDrive/validation-subset-vein-detection.zip' > /dev/null"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvN1s38RQedF"
      },
      "source": [
        "## 2. Load the pretrained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q4aLJKqxQ6iN"
      },
      "outputs": [],
      "source": [
        "# DefinCustomize writetemplate to write files from cells\n",
        "from IPython.core.magic import register_line_cell_magic\n",
        "\n",
        "@register_line_cell_magic\n",
        "def writetemplate(line, cell):\n",
        "    with open(line, 'w') as f:\n",
        "        f.write(cell.format(**globals()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YbDixxorQkc7"
      },
      "outputs": [],
      "source": [
        "# Define the model\n",
        "%%writetemplate /content/UnetResnet.py\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def conv3x3_bn(ci, co):\n",
        "    return torch.nn.Sequential(\n",
        "        torch.nn.Conv2d(ci, co, 3, padding=1),\n",
        "        torch.nn.BatchNorm2d(co),\n",
        "        torch.nn.ReLU(inplace=True)\n",
        "    )\n",
        "\n",
        "\n",
        "def encoder_conv(ci, co):\n",
        "    return torch.nn.Sequential(\n",
        "        torch.nn.MaxPool2d(2),\n",
        "        conv3x3_bn(ci, co),\n",
        "        conv3x3_bn(co, co),\n",
        "    )\n",
        "\n",
        "class deconv(torch.nn.Module):\n",
        "    def __init__(self, ci, co):\n",
        "        super(deconv, self).__init__()\n",
        "        self.upsample = torch.nn.ConvTranspose2d(ci, co, 2, stride=2)\n",
        "        self.conv1 = conv3x3_bn(ci, co)\n",
        "        self.conv2 = conv3x3_bn(co, co)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.upsample(x1)\n",
        "        diffX = x2.size()[2] - x1.size()[2]\n",
        "        diffY = x2.size()[3] - x1.size()[3]\n",
        "        x1 = F.pad(x1, (diffX, 0, diffY, 0))\n",
        "        # concatenamos los tensores\n",
        "        x = torch.cat([x2, x1], dim=1)\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class out_conv(torch.nn.Module):\n",
        "    def __init__(self, ci, co, coo):\n",
        "        super(out_conv, self).__init__()\n",
        "        self.upsample = torch.nn.ConvTranspose2d(ci, co, 2, stride=2)\n",
        "        self.conv = conv3x3_bn(ci, co)\n",
        "        self.final = torch.nn.Conv2d(co, coo, 1)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.upsample(x1)\n",
        "        diffX = x2.size()[2] - x1.size()[2]\n",
        "        diffY = x2.size()[3] - x1.size()[3]\n",
        "        x1 = F.pad(x1, (diffX, 0, diffY, 0))\n",
        "        x = self.conv(x1)\n",
        "        x = self.final(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class UNetResnet(torch.nn.Module):\n",
        "    def __init__(self, n_classes=3, in_ch=1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = torchvision.models.resnet18(pretrained=True)\n",
        "        if in_ch != 3:\n",
        "            self.encoder.conv1 = torch.nn.Conv2d(in_ch, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "\n",
        "        self.deconv1 = deconv(512, 256)\n",
        "        self.deconv2 = deconv(256, 128)\n",
        "        self.deconv3 = deconv(128, 64)\n",
        "        self.out = out_conv(64, 64, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_in = torch.tensor(x.clone())\n",
        "        x = self.encoder.relu(self.encoder.bn1(self.encoder.conv1(x)))\n",
        "        x1 = self.encoder.layer1(x)\n",
        "        x2 = self.encoder.layer2(x1)\n",
        "        x3 = self.encoder.layer3(x2)\n",
        "        x = self.encoder.layer4(x3)\n",
        "        x = self.deconv1(x, x3)\n",
        "        x = self.deconv2(x, x2)\n",
        "        x = self.deconv3(x, x1)\n",
        "        x = self.out(x, x_in)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "5dec4bf562a64a998bc3b1c007a634a7",
            "52945bf399304100bb2ad3c1197bb528",
            "acca6e4762894bb39e54d52bb60e720b",
            "f401686b866b40cdb5ef38b24f8cbe97",
            "03c8a7b2fac04559a2694e79f91fc576",
            "e451a81600a44473954a8d80241eb3d9",
            "648c10420bfc4b87b48dec9a7ed44e5e",
            "ab02514c8b4e46d6a7052cbfcf27604f",
            "2735ed95c3ed4f49b78d1e3e4c78e203",
            "28275e5120544c7181de3d65c178b696",
            "14a6ec0431a9410d9fbfacfca00dde91"
          ]
        },
        "id": "9R3KMD5lRE8G",
        "outputId": "711c1d87-1f65-44fa-ca18-b0f7bb791bdf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5dec4bf562a64a998bc3b1c007a634a7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0.00/44.7M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "UNetResnet(\n",
              "  (encoder): ResNet(\n",
              "    (conv1): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu): ReLU(inplace=True)\n",
              "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "    (layer1): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (layer2): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (layer3): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (layer4): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "    (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
              "  )\n",
              "  (deconv1): deconv(\n",
              "    (upsample): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
              "    (conv1): Sequential(\n",
              "      (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU(inplace=True)\n",
              "    )\n",
              "    (conv2): Sequential(\n",
              "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (deconv2): deconv(\n",
              "    (upsample): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
              "    (conv1): Sequential(\n",
              "      (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU(inplace=True)\n",
              "    )\n",
              "    (conv2): Sequential(\n",
              "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (deconv3): deconv(\n",
              "    (upsample): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
              "    (conv1): Sequential(\n",
              "      (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU(inplace=True)\n",
              "    )\n",
              "    (conv2): Sequential(\n",
              "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (out): out_conv(\n",
              "    (upsample): ConvTranspose2d(64, 64, kernel_size=(2, 2), stride=(2, 2))\n",
              "    (conv): Sequential(\n",
              "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU(inplace=True)\n",
              "    )\n",
              "    (final): Conv2d(64, 3, kernel_size=(1, 1), stride=(1, 1))\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from UnetResnet import UNetResnet\n",
        "\n",
        "checkpoint = torch.load('/content/gdrive/MyDrive/modelo_final.pth',map_location=torch.device('cpu'))\n",
        "model = UNetResnet()\n",
        "model.load_state_dict(checkpoint)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kUzS1JXQSJF"
      },
      "source": [
        "## 3. Get predicted masks for the validation subset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121,
          "referenced_widgets": [
            "96e83df8b96c4e029b5391f85e580152",
            "3e214542f373440e8130e38c3b02fadf",
            "dc6776d989934dfcb3c3ca254df660e3",
            "dfd3c4b7b3e94de9ba1f399b1465eb2a",
            "3c8045aea9d7403d82f1171841a2404e",
            "f181d456aa1449a399c4d466425387a9",
            "a2d5d97f73af4387a1300e48558e5edb",
            "b564baa592454b24bafd50802cfa3700",
            "48dba6f09d0e47738ed62e67506ad86c",
            "ea80a23c43114bebb0b17a144dc2af24",
            "c92a377d3b1644a8b4d7a61a00764f9e"
          ]
        },
        "id": "pSFk-8mRq-ZK",
        "outputId": "f422edce-6429-4ac2-8288-fb4331c5b89a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘/content/validation_masks’: File exists\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "96e83df8b96c4e029b5391f85e580152",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/384 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/content/UnetResnet.py:71: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x_in = torch.tensor(x.clone())\n"
          ]
        }
      ],
      "source": [
        "# Folder with the masks predicted by the UnetResnet model\n",
        "VALIDATION_MASKS_FOLDER = \"/content/validation_masks\"\n",
        "# Folder with the base images that haven't been preprocessed\n",
        "VALIDATION_IMAGES_FOLDER = \"/content/validation\"\n",
        "\n",
        "# Check if the folder exists\n",
        "if os.path.exists(VALIDATION_MASKS_FOLDER):\n",
        "  !rm -R \"$VALIDATION_MASKS_FOLDER\"\n",
        "  os.makedirs(VALIDATION_MASKS_FOLDER)\n",
        "else:\n",
        "  os.makedirs(VALIDATION_MASKS_FOLDER)\n",
        "\n",
        "!mkdir \"$VALIDATION_MASKS_FOLDER\"\n",
        "\n",
        "for image in tqdm(os.listdir(VALIDATION_IMAGES_FOLDER)):\n",
        "  image_file_name = image\n",
        "  image = cv2.imread(VALIDATION_IMAGES_FOLDER+\"/\"+image,0)\n",
        "\n",
        "  # Apply CLAHE\n",
        "  clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))\n",
        "  clahe_img = clahe.apply(image)\n",
        "\n",
        "  # Normalise frame\n",
        "  img = cv2.resize(clahe_img, (700,394), interpolation = cv2.INTER_AREA)\n",
        "  center = img.shape[1]//2\n",
        "  img = img[:,center-197:center+197].astype(\"float32\")/255\n",
        "\n",
        "  # Predict mask with background, arm, and vein segments\n",
        "  with torch.no_grad():\n",
        "      image = torch.from_numpy(img).unsqueeze(0)\n",
        "      image = image.to(device)\n",
        "      output = model(image.unsqueeze(0))[0]\n",
        "      pred_mask = torch.argmax(output, axis=0).squeeze().cpu().numpy()\n",
        "\n",
        "  # Polish the output mask's format\n",
        "  pred_mask = pred_mask.astype(np.uint8)\n",
        "  pred_mask[pred_mask==1] = 0\n",
        "  pred_mask[pred_mask==2] = 1\n",
        "  output = cv2.cvtColor(pred_mask, cv2.COLOR_GRAY2RGB)\n",
        "\n",
        "  # Write the predicted mask\n",
        "  cv2.imwrite(VALIDATION_MASKS_FOLDER+\"/\"+image_file_name, output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJb7mJ3mUOr7"
      },
      "source": [
        "## 4. Calculate the accuracy of the model predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182,
          "referenced_widgets": [
            "6527ec87d2d74226b00dffd184cd0d40",
            "b61922b2eead42c39305db30e59e5663",
            "038990646aa34500b46616382e25d9ba",
            "c1207a34a8ee41e494e3cfa88b6b33e7",
            "75417a6a2013491e91168ea29d556624",
            "69166a281d4e46db988237bd4136949c",
            "596b02edb00042aeb27d2d07ec906bf1",
            "443b722e37914cfcbdca8398b9531810",
            "38f34b60068e4cecbebe2ef56e44beab",
            "312f3209d261404faa61b51bb68baf02",
            "25542efe8b404a9b98e91660d2ae6fb0",
            "738a05c2284c4cfb8e31746f7a5bda22",
            "79c5f49a49a74a4986171d9a63dbf614",
            "765a6b0cfe6348bb8248c2638c7c5e86",
            "529ac3b437cc43068e9f85285f8ff052",
            "051cc83df9eb457799b5c291823d649d",
            "aa2b0f6861b347f0b4b2772b767be07f",
            "10d97d9b332f4d5bbfad082c7cce5b4d",
            "d09fd9acac0e4ae1aa112b9015be3677",
            "fdff8dc8b16c4250818fd7bd0cd667ad",
            "fcd82c645f8746f5894021d16b015e01",
            "c4e639d5a38b4c26a80e9a4ea2783d83",
            "c8b783fb10374f5f9d736cb819bfe661",
            "41ff886def7140ad864ebc3a3f776c4c",
            "f9cc7993146c40fcb924f4e52bd2dea6",
            "f949470664e744f4b0051b0b876e2615",
            "14d6ea1803bc4c1e953fc4b455e5ec3e",
            "486054d6415d4b7cadff3a2f2163a330",
            "1fd2e89c948a4835a11a527f15ac989b",
            "676669aa1a24424fa713f3f7f191229a",
            "23517672612e4c9d862a558cf368df09",
            "03bd61ba01ed44bcb415f366bcfe90b5",
            "4e8a04f4e9d041c59a0ed37b8185fba5"
          ]
        },
        "id": "GGCEqs8OwLBa",
        "outputId": "974d09f4-b182-46d8-e9bd-3054c7fd9461"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6527ec87d2d74226b00dffd184cd0d40",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/384 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:31: RuntimeWarning: invalid value encountered in ulong_scalars\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "738a05c2284c4cfb8e31746f7a5bda22",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/384 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c8b783fb10374f5f9d736cb819bfe661",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/384 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validación Aleyda Herrera:  0.8154981549815498\n",
            "Validación Danithsa Garron:  0.784375\n",
            "Validación Paula Galindo:  0.886986301369863\n"
          ]
        }
      ],
      "source": [
        "def validate(lower_threshold, upper_threshold, set_path):\n",
        "  '''\n",
        "    This function's main purpose is to find the total percentage of the intersection of annotated dots\n",
        "    inside predicted segments, which are predicted by the UnetResnet model.\n",
        "  '''\n",
        "  intersections = []\n",
        "\n",
        "  for image_path in tqdm(os.listdir(set_path), total = len(os.listdir(set_path))):\n",
        "    # Open annotated image\n",
        "    image = cv2.imread(set_path+image_path)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Apply colour thresholding to the annotated image\n",
        "    mask = cv2.inRange(image, lower_threshold, upper_threshold)\n",
        "\n",
        "    # Resize and restructure the annotated image to compare it with a predicted mask\n",
        "    img = cv2.resize(mask, (700,394), interpolation = cv2.INTER_AREA)\n",
        "    center = img.shape[1]//2\n",
        "    mask = img[:,center-197:center+197]\n",
        "    ret, mask = cv2.threshold(mask,127,200,cv2.THRESH_BINARY)\n",
        "    mask[mask==200] = 1\n",
        "\n",
        "    # Read the corresponding predicted mask (coming from the model)\n",
        "    pred_mask = cv2.imread(\"/content/validation_masks/\"+image_path, 0)\n",
        "\n",
        "    # Find the intersection between the annotated image (which is now a mask) and the predicted mask\n",
        "    intersection = cv2.bitwise_and(mask, pred_mask)\n",
        "\n",
        "    # Find the percentage of good predicted pixels (ideal venupuncture dots) over the total\n",
        "    # of annotated venopuncture pixels in the image (annotated by the professionals)\n",
        "    sum = np.sum(intersection) / np.sum(mask)\n",
        "\n",
        "    # Append the percentages to the intersections list\n",
        "    intersections.append(sum)\n",
        "\n",
        "  # Return the mean percentage for the given subset\n",
        "  my_list = np.array(intersections)\n",
        "  return np.mean(my_list[np.isfinite(my_list)])\n",
        "\n",
        "result_herrera = validate(lower_threshold=np.array([200,0,0]), upper_threshold=np.array([255,180,180]), set_path=\"/content/valid_Aleyda_Herrera/\")\n",
        "result_galindo = validate(lower_threshold=np.array([200,0,0]), upper_threshold=np.array([255,180,180]), set_path=\"/content/valid_Paula_Galindo/\")\n",
        "result_garron = validate(lower_threshold=np.array([200,0,0]), upper_threshold=np.array([255,180,180]), set_path=\"/content/valid_Danithsa_Garron/\")\n",
        "\n",
        "print(\"Validation Aleyda Herrera: \", result_herrera)\n",
        "print(\"Validation Danithsa Garron: \", result_garron)\n",
        "print(\"Validation Paula Galindo: \", result_galindo)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
